{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: stephen.marquard@uct.ac.za\n",
      "From: louis@media.berkeley.edu\n",
      "From: zqian@umich.edu\n",
      "From: rjlowe@iupui.edu\n",
      "From: zqian@umich.edu\n",
      "From: rjlowe@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: gsilver@umich.edu\n",
      "From: gsilver@umich.edu\n",
      "From: zqian@umich.edu\n",
      "From: gsilver@umich.edu\n",
      "From: wagnermr@iupui.edu\n",
      "From: zqian@umich.edu\n",
      "From: antranig@caret.cam.ac.uk\n",
      "From: gopal.ramasammycook@gmail.com\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: stephen.marquard@uct.ac.za\n",
      "From: louis@media.berkeley.edu\n",
      "From: louis@media.berkeley.edu\n",
      "From: ray@media.berkeley.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n"
     ]
    }
   ],
   "source": [
    "# Search for lines that start with 'F', followed by\n",
    "# 2 characters, followed by 'm:'\n",
    "import re\n",
    "hand = open('mbox-short.txt')\n",
    "for line in hand:\n",
    "    line = line.rstrip()\n",
    "    if re.search('^F..m:', line):\n",
    "       # print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: stephen.marquard@uct.ac.za\n",
      "From: louis@media.berkeley.edu\n",
      "From: zqian@umich.edu\n",
      "From: rjlowe@iupui.edu\n",
      "From: zqian@umich.edu\n",
      "From: rjlowe@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: gsilver@umich.edu\n",
      "From: gsilver@umich.edu\n",
      "From: zqian@umich.edu\n",
      "From: gsilver@umich.edu\n",
      "From: wagnermr@iupui.edu\n",
      "From: zqian@umich.edu\n",
      "From: antranig@caret.cam.ac.uk\n",
      "From: gopal.ramasammycook@gmail.com\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: stephen.marquard@uct.ac.za\n",
      "From: louis@media.berkeley.edu\n",
      "From: louis@media.berkeley.edu\n",
      "From: ray@media.berkeley.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n"
     ]
    }
   ],
   "source": [
    "# Search for lines that start with From and have an at sign\n",
    "import re\n",
    "hand = open('mbox-short.txt')\n",
    "for line in hand:\n",
    "    line = line.rstrip()\n",
    "    if re.search('^From:.+@', line):\n",
    "       # print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re ##Extracting data with findall\n",
    "x = 'My 2 favorite nmbers are 19 and 42'\n",
    "y = re.findall('[0-9]+', x)#( + one or more digits)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: Using the :']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "x = 'FROM: Using the : character'\n",
    "y = re.findall('^F.+:', x)# starts with F followed by . any character + one or more times last character is : but Greedy \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM:']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "x = 'FROM: Using the : character'\n",
    "y = re.findall('^F.+?:', x)# starts ^ with F followed by . any character + one or more times last character ? is : but non-Greedy\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are looking for substrings that have at least\n",
    "one non-whitespace character, followed by an at-sign, followed by at least one more\n",
    "non-whitespace character. The \\S+ matches as many non-whitespace characters\n",
    "as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stephen.marquard@uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "x = 'From stephen.marquard@uct.ac.za Sat Jan  5 09:14:16 2008'## a \\S non blank character one or more times + folowed by non blank character \\S\n",
    "y = re.findall('\\S+@\\S+', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stephen.marquard@uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "x = 'From stephen.marquard@uct.ac.za Sat Jan  5 09:14:16 2008'## a \\S non blank character one or more times + folowed by non blank character \\S\n",
    "y = re.findall('^From (\\S+@\\S+) ', x) #i am telling start extracting after the space\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stephen.marquard@uct.ac.za']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801051412.m05ECIaH010327@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['stephen.marquard@uct.ac.za']\n",
      "['source@collab.sakaiproject.org']\n",
      "['stephen.marquard@uct.ac.za']\n",
      "['stephen.marquard@uct.ac.za']\n",
      "['louis@media.berkeley.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801042308.m04N8v6O008125@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['louis@media.berkeley.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['louis@media.berkeley.edu']\n",
      "['louis@media.berkeley.edu']\n",
      "['zqian@umich.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801042109.m04L92hb007923@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['zqian@umich.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['zqian@umich.edu']\n",
      "['zqian@umich.edu']\n",
      "['rjlowe@iupui.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801042044.m04Kiem3007881@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['rjlowe@iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['rjlowe@iupui.edu']\n",
      "['rjlowe@iupui.edu']\n",
      "['zqian@umich.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801042001.m04K1cO0007738@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['zqian@umich.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['zqian@umich.edu']\n",
      "['zqian@umich.edu']\n",
      "['zqian@umich.edu']\n",
      "['rjlowe@iupui.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801041948.m04JmdwO007705@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['rjlowe@iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['rjlowe@iupui.edu']\n",
      "['rjlowe@iupui.edu']\n",
      "['cwen@iupui.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801041635.m04GZQGZ007313@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['cwen@iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['cwen@iupui.edu']\n",
      "['cwen@iupui.edu']\n",
      "['hu2@iupui.edu']\n",
      "['cwen@iupui.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801041633.m04GX6eG007292@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['cwen@iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['cwen@iupui.edu']\n",
      "['cwen@iupui.edu']\n",
      "['hu2@iupui.edu']\n",
      "['gsilver@umich.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801041611.m04GB1Lb007221@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['gsilver@umich.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['gsilver@umich.edu']\n",
      "['gsilver@umich.edu']\n",
      "['gsilver@umich.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801041610.m04GA5KP007209@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['gsilver@umich.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['gsilver@umich.edu']\n",
      "['gsilver@umich.edu']\n",
      "['zqian@umich.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801041609.m04G9EuX007197@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['zqian@umich.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['zqian@umich.edu']\n",
      "['zqian@umich.edu']\n",
      "['gsilver@umich.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801041608.m04G8d7w007184@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['gsilver@umich.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['gsilver@umich.edu']\n",
      "['gsilver@umich.edu']\n",
      "['wagnermr@iupui.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801041537.m04Fb6Ci007092@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['wagnermr@iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['wagnermr@iupui.edu']\n",
      "['wagnermr@iupui.edu']\n",
      "['zqian@umich.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801041515.m04FFv42007050@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['zqian@umich.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['zqian@umich.edu']\n",
      "['zqian@umich.edu']\n",
      "['antranig@caret.cam.ac.uk']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801041502.m04F21Jo007031@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['antranig@caret.cam.ac.uk']\n",
      "['source@collab.sakaiproject.org']\n",
      "['antranig@caret.cam.ac.uk']\n",
      "['antranig@caret.cam.ac.uk']\n",
      "['gopal.ramasammycook@gmail.com']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801041403.m04E3psW006926@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['gopal.ramasammycook@gmail.com']\n",
      "['source@collab.sakaiproject.org']\n",
      "['gopal.ramasammycook@gmail.com']\n",
      "['gopal.ramasammycook@gmail.com']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801041200.m04C0gfK006793@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['source@collab.sakaiproject.org']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['dhorwitz@david-horwitz-6:~/branchManagemnt/sakai_2-5-x']\n",
      "['dhorwitz@david-horwitz-6:~/branchManagemnt/sakai_2-5-x']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801041106.m04B6lK3006677@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['source@collab.sakaiproject.org']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801040947.m049lUxo006517@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['source@collab.sakaiproject.org']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['josrodri@iupui.edu']\n",
      "['dhorwitz@david-horwitz-6:~/branchManagemnt/sakai_2-5-x']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801040932.m049W2i5006493@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['source@collab.sakaiproject.org']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['david.horwitz@uct.ac.za']\n",
      "['josrodri@iupui.edu']\n",
      "['dhorwitz@david-horwitz-6:~/branchManagemnt/sakai_2-5-x']\n",
      "['stephen.marquard@uct.ac.za']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801040905.m0495rWB006420@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['stephen.marquard@uct.ac.za']\n",
      "['source@collab.sakaiproject.org']\n",
      "['stephen.marquard@uct.ac.za']\n",
      "['stephen.marquard@uct.ac.za']\n",
      "['louis@media.berkeley.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801040023.m040NpCc005473@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['louis@media.berkeley.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['louis@media.berkeley.edu']\n",
      "['louis@media.berkeley.edu']\n",
      "['louis@media.berkeley.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801032216.m03MGhDa005292@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['louis@media.berkeley.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['louis@media.berkeley.edu']\n",
      "['louis@media.berkeley.edu']\n",
      "['ray@media.berkeley.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801032205.m03M5Ea7005273@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['ray@media.berkeley.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['ray@media.berkeley.edu']\n",
      "['ray@media.berkeley.edu']\n",
      "['cwen@iupui.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801032133.m03LX3gG005191@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['cwen@iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['cwen@iupui.edu']\n",
      "['cwen@iupui.edu']\n",
      "['cwen@iupui.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801032127.m03LRUqH005177@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['cwen@iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['cwen@iupui.edu']\n",
      "['cwen@iupui.edu']\n",
      "['wagnermr@iupui.edu']\n",
      "['cwen@iupui.edu']\n",
      "['postmaster@collab.sakaiproject.org']\n",
      "['200801032122.m03LMFo4005148@nakamura.uits.iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['source@collab.sakaiproject.org']\n",
      "['apache@localhost']\n",
      "['source@collab.sakaiproject.org']\n",
      "['cwen@iupui.edu']\n",
      "['source@collab.sakaiproject.org']\n",
      "['cwen@iupui.edu']\n",
      "['cwen@iupui.edu']\n",
      "['wagnermr@iupui.edu']\n"
     ]
    }
   ],
   "source": [
    "# Search for lines that have an at sign between characters\n",
    "# The characters must be a letter or number\n",
    "import re\n",
    "hand = open('mbox-short.txt')\n",
    "for line in hand:\n",
    "    line = line.rstrip()\n",
    "    x = re.findall('[a-zA-Z0-9]\\S+@\\S+[a-zA-Z]', line)\n",
    "    if len(x) > 0:\n",
    "        #print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "31\n",
      "uct.ac.za\n"
     ]
    }
   ],
   "source": [
    "data = 'From stephen.marquard@uct.ac.za Sat Jan  5 09:14:16 2008'\n",
    "atpos = data.find('@')\n",
    "print(atpos)\n",
    "#\n",
    "sppos = data.find(' ', atpos)#starts from 21\n",
    "print(sppos)\n",
    "#\n",
    "host = data[atpos+1 : sppos]\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uct.ac.za\n"
     ]
    }
   ],
   "source": [
    "line = 'From stephen.marquard@uct.ac.za Sat Jan  5 09:14:16 2008'\n",
    "words = line.split()\n",
    "email = words[1]\n",
    "pieces = email.split('@')\n",
    "print(pieces[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "line = 'From stephen.marquard@uct.ac.za Sat Jan  5 09:14:16 2008'\n",
    "y = re.findall('@([^ ]*)', line)#[]match non-blank character ; * match many of them ; ^inside [^  ] means everything but space\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "y = re.findall('^From .*@([^ ]*)', line)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum: 0.9907\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "hand = open('mbox-short.txt')\n",
    "numlist = list()\n",
    "for line in hand :\n",
    "    line = line.rstrip()\n",
    "    stuff = re.findall('^X-DSPAM-Confidence: ([0-9.]+)', line)# [0-9.] a digit or period\n",
    "    if len(stuff) != 1 : continue\n",
    "    num = float(stuff[0])\n",
    "    numlist.append(num)\n",
    "\n",
    "print('Maximum:', max(numlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we want lines that start with X-, followed by zero\n",
    "or more characters (.*), followed by a colon (:) and then a space. After the\n",
    "space we are looking for one or more characters that are either a digit (0-9) or\n",
    "a period [0-9.]+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum: 0.9907\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "hand = open('mbox-short.txt')\n",
    "numlist = list()\n",
    "for line in hand :\n",
    "    line = line.rstrip()\n",
    "    stuff = re.findall('^X-.*: ([0-9.]+)', line)# [0-9.] a digit or period\n",
    "    if len(stuff) != 1 : continue\n",
    "    num = float(stuff[0])\n",
    "    numlist.append(num)\n",
    "\n",
    "print('Maximum:', max(numlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-DSPAM-Confidence: 0.8475\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.6178\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.6961\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.7565\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.7626\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.7556\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.7002\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.7615\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.7601\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.7605\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.6959\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.7606\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.7559\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.7605\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.6932\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.7558\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.6526\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.6948\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.6528\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.7002\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.7554\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.6956\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.6959\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.7556\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.9846\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.8509\n",
      "X-DSPAM-Probability: 0.0000\n",
      "X-DSPAM-Confidence: 0.9907\n",
      "X-DSPAM-Probability: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Search for lines that start with 'X' followed by any non\n",
    "# whitespace characters and ':'\n",
    "# followed by a space and any number.\n",
    "# The number can include a decimal.\n",
    "\n",
    "import re\n",
    "hand = open('mbox-short.txt')\n",
    "for line in hand:\n",
    "    line = line.rstrip()\n",
    "    if re.search('^X\\S*: [0-9.]+', line):\n",
    "       # print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8475\n",
      "0.0000\n",
      "0.6178\n",
      "0.0000\n",
      "0.6961\n",
      "0.0000\n",
      "0.7565\n",
      "0.0000\n",
      "0.7626\n",
      "0.0000\n",
      "0.7556\n",
      "0.0000\n",
      "0.7002\n",
      "0.0000\n",
      "0.7615\n",
      "0.0000\n",
      "0.7601\n",
      "0.0000\n",
      "0.7605\n",
      "0.0000\n",
      "0.6959\n",
      "0.0000\n",
      "0.7606\n",
      "0.0000\n",
      "0.7559\n",
      "0.0000\n",
      "0.7605\n",
      "0.0000\n",
      "0.6932\n",
      "0.0000\n",
      "0.7558\n",
      "0.0000\n",
      "0.6526\n",
      "0.0000\n",
      "0.6948\n",
      "0.0000\n",
      "0.6528\n",
      "0.0000\n",
      "0.7002\n",
      "0.0000\n",
      "0.7554\n",
      "0.0000\n",
      "0.6956\n",
      "0.0000\n",
      "0.6959\n",
      "0.0000\n",
      "0.7556\n",
      "0.0000\n",
      "0.9846\n",
      "0.0000\n",
      "0.8509\n",
      "0.0000\n",
      "0.9907\n",
      "0.0000\n"
     ]
    }
   ],
   "source": [
    "# Search for lines that start with 'X' followed by any\n",
    "# non whitespace characters and ':' followed by a space\n",
    "# and any number. The number can include a decimal.\n",
    "# Then print the number if it is greater than zero.\n",
    "import re\n",
    "hand = open('mbox-short.txt')\n",
    "for line in hand:\n",
    "    line = line.rstrip()\n",
    "    x = re.findall('^X\\S*: ([0-9.]+)', line)\n",
    "    if len(x) > 0:\n",
    "        #print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating our regular expression, we are looking for lines that start with\n",
    "Details:, followed by any number of characters (.*), followed by rev=, and then\n",
    "by one or more digits. We want to find lines that match the entire expression but\n",
    "we only want to extract the integer number at the end of the line, so we surround\n",
    "[0-9]+ with parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['39772']\n",
      "['39771']\n",
      "['39770']\n",
      "['39769']\n",
      "['39766']\n",
      "['39765']\n",
      "['39764']\n",
      "['39763']\n",
      "['39762']\n",
      "['39761']\n",
      "['39760']\n",
      "['39759']\n",
      "['39758']\n",
      "['39757']\n",
      "['39756']\n",
      "['39755']\n",
      "['39754']\n",
      "['39753']\n",
      "['39752']\n",
      "['39751']\n",
      "['39750']\n",
      "['39749']\n",
      "['39746']\n",
      "['39745']\n",
      "['39744']\n",
      "['39743']\n",
      "['39742']\n"
     ]
    }
   ],
   "source": [
    "# Search for lines that start with 'Details: rev='\n",
    "# followed by numbers and '.'\n",
    "# Then print the number if it is greater than zero\n",
    "import re\n",
    "hand = open('mbox-short.txt')\n",
    "for line in hand:\n",
    "    line = line.rstrip()\n",
    "    x = re.findall('^Details:.*rev=([0-9.]+)', line)\n",
    "    if len(x) > 0:\n",
    "       # print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are looking for lines that start\n",
    "with From (note the space), followed by any number of characters (.*), followed by\n",
    "a space, followed by two digits [0-9][0-9], followed by a colon character. This is\n",
    "the definition of the kinds of lines we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-15-7bb16860ceaa>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-7bb16860ceaa>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    #print(x)\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# Search for lines that start with From and a character\n",
    "# followed by a two digit number between 00 and 99 followed by ':'\n",
    "# Then print the number if it is greater than zero\n",
    "import re\n",
    "hand = open('mbox-short.txt')\n",
    "for line in hand:\n",
    "    line = line.rstrip()\n",
    "    x = re.findall('^From .* ([0-9][0-9]):', line)\n",
    "    if len(x) > 0: \n",
    "        #print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$10.00\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "x = 'We just recieved $10.00 for cookies.'\n",
    "y = re.findall('\\$[0-9.]+', x)#the prefix'\\' to search for special regular expression as $\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: Using the :']\n"
     ]
    }
   ],
   "source": [
    "x = 'From: Using the : character'\n",
    "y = re.findall('^F.+:', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353946\n"
     ]
    }
   ],
   "source": [
    "#Assignment week 2\n",
    "import re \n",
    "file = open('regex_sum_373793.txt')\n",
    "numlist = list()\n",
    "for line in file :\n",
    "    line = line.rstrip()\n",
    "    stuff = re.findall('[0-9]+', line)     \n",
    "    if len(stuff) != 0 : \n",
    "        for num in stuff :\n",
    "            num = int(num)\n",
    "            numlist.append(num)\n",
    "print(sum(numlist))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-144-4ba18276d701>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-144-4ba18276d701>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    print( sum( [ ****** *** * in **********('[0-9]+',**************************.read()) ] ) )\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print( sum( [ ****** *** * in **********('[0-9]+',**************************.read()) ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networked programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the easiest way to show how the HTTP protocol works is to write a very\n",
    "simple Python program that makes a connection to a web server and follows the\n",
    "rules of the HTTP protocol to request a document and display what the server\n",
    "sends back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the program makes a connection to port 80 on the server www.py4e.com.\n",
    "Since our program is playing the role of the “web browser”, the HTTP protocol\n",
    "says we must send the GET command followed by a blank line. \\r\\n signifies\n",
    "an EOL (end of line), so \\r\\n\\r\\n signifies nothing between two EOL sequences.\n",
    "That is the equivalent of a blank line.\n",
    "Once we send that blank line, we write a loop that receives data in 512-character\n",
    "chunks from the socket and prints the data out until there is no more data to read\n",
    "(i.e., the recv() returns an empty string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r\n",
      "Date: Sat, 11 Apr 2020 07:56:11 GMT\r\n",
      "Server: Apache/2.4.18 (Ubuntu)\r\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\r\n",
      "ETag: \"a7-54f6609245537\"\r\n",
      "Accept-Ranges: bytes\r\n",
      "Content-Length: 167\r\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\r\n",
      "Pragma: no-cache\r\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\r\n",
      "Connection: close\r\n",
      "Content-Type: text/plain\r\n",
      "\r\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #1-make a socket\n",
    "mysock.connect(('data.pr4e.org', 80)) #2-connect the socket... host, port\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode() #convert strings into bytes objects\n",
    "mysock.send(cmd) #3-send a get request\n",
    "\n",
    "while True: #4-recieve the data\n",
    "    data = mysock.recv(512)\n",
    "    if(len(data) < 1) :\n",
    "        break\n",
    "    print(data.decode(),end='')#transfer it to character\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r\n",
      "Date: Sat, 11 Apr 2020 07:56:13 GMT\r\n",
      "Server: Apache/2.4.18 (Ubuntu)\r\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\r\n",
      "ETag: \"1d3-54f6609240717\"\r\n",
      "Accept-Ranges: bytes\r\n",
      "Content-Length: 467\r\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\r\n",
      "Pragma: no-cache\r\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\r\n",
      "Connection: close\r\n",
      "Content-Type: text/plain\r\n",
      "\r\n",
      "Why should you learn to write programs?\n",
      "\n",
      "Writing programs (or programming) is a very creative \n",
      "and rewarding activity.  You can write programs for \n",
      "many reasons, ranging from making your living to solving\n",
      "a difficult data analysis problem to having fun to helping\n",
      "someone else solve a problem.  This book assumes that \n",
      "everyone needs to know how to program, and that once \n",
      "you know how to program you will figure out what you want \n",
      "to do with your newfound skills.  \n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = b'GET http://data.pr4e.org/intro-short.txt HTTP/1.0\\r\\n\\r\\n'#encode() and b'' are equivalent\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(),end='')\n",
    "\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving web pages with urllib in Python\n",
    "While we can manually send and receive data over HTTP using the socket library,\n",
    "there is a much simpler way to perform this common task in Python by using the\n",
    "urllib library.\n",
    "Using urllib, you can treat a web page much like a file. You simply indicate\n",
    "which web page you would like to retrieve and urllib handles all of the HTTP\n",
    "protocol and header details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "#The equivalent code to read the romeo.txt file from the web using urllib is as follows:\n",
    "    \n",
    "import urllib.request as urlr\n",
    "\n",
    "fhand = urlr.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}\n"
     ]
    }
   ],
   "source": [
    "#we can write a program to retrieve the data for romeo.txt and compute the frequency of each word in the file as follows\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "\n",
    "counts = dict()\n",
    "for line in fhand:\n",
    "    words = line.decode().split()\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing HTML and scraping the web\n",
    "\n",
    "One of the common uses of the urllib capability in Python is to scrape the web.\n",
    "Web scraping is when we write a program that pretends to be a web browser and\n",
    "retrieves pages, then examines the data in those pages looking for patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing HTML using regular expressions\n",
    "\n",
    "Our regular expression looks for strings that start with “href=\"http://” or\n",
    "“href=\"https://”, followed by one or more characters (.+?), followed by another\n",
    "double quote. The question mark behind the [s]? indicates to search for the\n",
    "string “http” followed by zero or one “s”.\n",
    "The question mark added to the .+? indicates that the match is to be done in\n",
    "a “non-greedy” fashion instead of a “greedy” fashion. A non-greedy match tries\n",
    "to find the smallest possible matching string and a greedy match tries to find the\n",
    "largest possible matching string.\n",
    "We add parentheses to our regular expression to indicate which part of our matched\n",
    "string we would like to extract, and produce the following program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://www.dr-chuck.com/page2.htm\n"
     ]
    }
   ],
   "source": [
    "# Search for link values within URL input\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import re\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "links = re.findall(b'href=\"(http[s]?://.+?)\"', html)\n",
    "for link in links:\n",
    "    print(link.decode())\n",
    "    \n",
    "#The ssl library allows this program to access web sites that strictly enforce HTTPS.\n",
    "#The read method returns HTML source code as a bytes object instead of returning\n",
    "#an HTTPResponse object. The findall regular expression method will give us a\n",
    "#list of all of the strings that match our regular expression, returning only the link\n",
    "#text between the double quotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing HTML using BeautifulSoup\n",
    "\n",
    "We will use urllib to read the page and then use BeautifulSoup to extract the\n",
    "href attributes from the anchor (a) tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://www.dr-chuck.com/page2.htm\n",
      "page1.htm\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))\n",
    "\n",
    "#The program prompts for a web address, then opens the web page, reads the data\n",
    "#and passes the data to the BeautifulSoup parser, and then retrieves all of the\n",
    "#anchor tags and prints out the href attribute for each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://py4e-data.dr-chuck.net/comments_373795.html\n",
      "TAG: <span class=\"comments\">100</span>\n",
      "URL: None\n",
      "Contents: 100\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">100</span>\n",
      "URL: None\n",
      "Contents: 100\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">98</span>\n",
      "URL: None\n",
      "Contents: 98\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">93</span>\n",
      "URL: None\n",
      "Contents: 93\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">91</span>\n",
      "URL: None\n",
      "Contents: 91\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">87</span>\n",
      "URL: None\n",
      "Contents: 87\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">85</span>\n",
      "URL: None\n",
      "Contents: 85\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">78</span>\n",
      "URL: None\n",
      "Contents: 78\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">78</span>\n",
      "URL: None\n",
      "Contents: 78\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">77</span>\n",
      "URL: None\n",
      "Contents: 77\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">76</span>\n",
      "URL: None\n",
      "Contents: 76\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">76</span>\n",
      "URL: None\n",
      "Contents: 76\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">75</span>\n",
      "URL: None\n",
      "Contents: 75\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">71</span>\n",
      "URL: None\n",
      "Contents: 71\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">70</span>\n",
      "URL: None\n",
      "Contents: 70\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">69</span>\n",
      "URL: None\n",
      "Contents: 69\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">68</span>\n",
      "URL: None\n",
      "Contents: 68\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">68</span>\n",
      "URL: None\n",
      "Contents: 68\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">64</span>\n",
      "URL: None\n",
      "Contents: 64\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">61</span>\n",
      "URL: None\n",
      "Contents: 61\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">55</span>\n",
      "URL: None\n",
      "Contents: 55\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">46</span>\n",
      "URL: None\n",
      "Contents: 46\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">44</span>\n",
      "URL: None\n",
      "Contents: 44\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">41</span>\n",
      "URL: None\n",
      "Contents: 41\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">41</span>\n",
      "URL: None\n",
      "Contents: 41\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">40</span>\n",
      "URL: None\n",
      "Contents: 40\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">36</span>\n",
      "URL: None\n",
      "Contents: 36\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">35</span>\n",
      "URL: None\n",
      "Contents: 35\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">31</span>\n",
      "URL: None\n",
      "Contents: 31\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">28</span>\n",
      "URL: None\n",
      "Contents: 28\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">25</span>\n",
      "URL: None\n",
      "Contents: 25\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">24</span>\n",
      "URL: None\n",
      "Contents: 24\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">23</span>\n",
      "URL: None\n",
      "Contents: 23\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">23</span>\n",
      "URL: None\n",
      "Contents: 23\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">23</span>\n",
      "URL: None\n",
      "Contents: 23\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">19</span>\n",
      "URL: None\n",
      "Contents: 19\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">18</span>\n",
      "URL: None\n",
      "Contents: 18\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">17</span>\n",
      "URL: None\n",
      "Contents: 17\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">15</span>\n",
      "URL: None\n",
      "Contents: 15\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">15</span>\n",
      "URL: None\n",
      "Contents: 15\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">13</span>\n",
      "URL: None\n",
      "Contents: 13\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">13</span>\n",
      "URL: None\n",
      "Contents: 13\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">12</span>\n",
      "URL: None\n",
      "Contents: 12\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">10</span>\n",
      "URL: None\n",
      "Contents: 10\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">8</span>\n",
      "URL: None\n",
      "Contents: 8\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">7</span>\n",
      "URL: None\n",
      "Contents: 7\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">5</span>\n",
      "URL: None\n",
      "Contents: 5\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">4</span>\n",
      "URL: None\n",
      "Contents: 4\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">2</span>\n",
      "URL: None\n",
      "Contents: 2\n",
      "Attrs: {'class': ['comments']}\n",
      "TAG: <span class=\"comments\">1</span>\n",
      "URL: None\n",
      "Contents: 1\n",
      "Attrs: {'class': ['comments']}\n"
     ]
    }
   ],
   "source": [
    "#You can use also BeautifulSoup to pull out various parts of each tag:\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all of the span tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    print('TAG:', tag)\n",
    "    print('URL:', tag.get('href', None))\n",
    "    print('Contents:', tag.contents[0])\n",
    "    print('Attrs:', tag.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 50\n",
      "Sum 2259\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "numlist = list()\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "html = urlopen(\"http://py4e-data.dr-chuck.net/comments_373795.html\", context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all of the span tags\n",
    "tags = soup('span')\n",
    "for tag in tags:\n",
    "    num = int(tag.contents[0])\n",
    "    numlist.append(num)\n",
    "print('Count', len(numlist))\n",
    "print('Sum', sum(numlist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2259\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import re \n",
    "sum = 0\n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "html = urlopen(\"http://py4e-data.dr-chuck.net/comments_373795.html\", context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "\n",
    "tags=soup('span')\n",
    "\n",
    "for tag in tags:\n",
    "    sum=sum+int(tag.contents[0])\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment: Following Links in HTML Using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://py4e-data.dr-chuck.net/known_by_Rameesah.html\n",
      "Enter count: 7\n",
      "Enter position: 18\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Neo.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Sajid.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Conlon.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Cosmo.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Oisin.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Lochlann.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Faria.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Faria.html\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "urllist = list()\n",
    "url = input('Enter - ')\n",
    "count = int(input('Enter count: '))\n",
    "pos = int(input('Enter position: ')) - 1\n",
    "\n",
    "for i in range(count):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tags=soup('a')\n",
    "    url = tags[pos].get('href', None) \n",
    "    print('Retrieving: ', url)\n",
    "    urllist.append(url)\n",
    "    \n",
    "\n",
    "print('Retrieving: ', urllist[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Url: http://py4e-data.dr-chuck.net/known_by_Rameesah.html\n",
      "Enter count: 7\n",
      "Enter position:18\n",
      "http://py4e-data.dr-chuck.net/known_by_Neo.html\n",
      "Neo\n",
      "http://py4e-data.dr-chuck.net/known_by_Sajid.html\n",
      "Sajid\n",
      "http://py4e-data.dr-chuck.net/known_by_Conlon.html\n",
      "Conlon\n",
      "http://py4e-data.dr-chuck.net/known_by_Cosmo.html\n",
      "Cosmo\n",
      "http://py4e-data.dr-chuck.net/known_by_Oisin.html\n",
      "Oisin\n",
      "http://py4e-data.dr-chuck.net/known_by_Lochlann.html\n",
      "Lochlann\n",
      "http://py4e-data.dr-chuck.net/known_by_Faria.html\n",
      "Faria\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "url = input('Enter Url: ')\n",
    "count = int(input(\"Enter count: \"))\n",
    "position = int(input(\"Enter position:\"))\n",
    "for i in range(count):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    tags = soup('a')\n",
    "    s = []\n",
    "    t = []\n",
    "    for tag in tags:\n",
    "        x = tag.get('href', None)\n",
    "        s.append(x)\n",
    "        y = tag.text\n",
    "        t.append(y)\n",
    "    \n",
    "    print(s[position-1])\n",
    "    print(t[position-1])\n",
    "    url = s[position-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data on the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing XML in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Chuck\n",
      "Attr: yes\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "data = '''              #...multi line string\n",
    "<person>\n",
    "  <name>Chuck</name>\n",
    "  <phone type=\"intl\">\n",
    "    +1 734 303 4456\n",
    "  </phone>\n",
    "  <email hide=\"yes\"/>\n",
    "</person>'''\n",
    "\n",
    "tree = ET.fromstring(data)#it gives us a tree object from a string\n",
    "print('Name:', tree.find('name').text)#to get the text within the name\n",
    "print('Attr:', tree.find('email').get('hide'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User count: 2\n",
      "Name Chuck\n",
      "Id 001\n",
      "Attribute 2\n",
      "Name Brent\n",
      "Id 009\n",
      "Attribute 7\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "input = '''\n",
    "<stuff>\n",
    "  <users>\n",
    "    <user x=\"2\">\n",
    "      <id>001</id>\n",
    "      <name>Chuck</name>\n",
    "    </user>\n",
    "    <user x=\"7\">\n",
    "      <id>009</id>\n",
    "      <name>Brent</name>\n",
    "    </user>\n",
    "  </users>\n",
    "</stuff>'''\n",
    "\n",
    "stuff = ET.fromstring(input)\n",
    "lst = stuff.findall('users/user')#to find all user under users. It gets all tags in one list\n",
    "print('User count:', len(lst))\n",
    "\n",
    "for item in lst:\n",
    "    print('Name', item.find('name').text)\n",
    "    print('Id', item.find('id').text)\n",
    "    print('Attribute', item.get('x'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML Assignment\n",
    "In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/geoxml.py. The program will prompt for a URL, read the XML data from that URL using urllib and then parse and extract the comment counts from the XML data, compute the sum of the numbers in the file.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/comments_42.xml (Sum=2553)\n",
    "\n",
    "You are to look through all the <comment> tags and find the <count> values sum the numbers. The closest sample code that shows how to parse XML is geoxml.py. But since the nesting of the elements in our data is different than the data we are parsing in that sample code you will have to make real changes to the code.\n",
    "To make the code a little simpler, you can use an XPath selector string to look through the entire tree of XML for any tag named 'count' with the following line of code:\n",
    "\n",
    "counts = tree.findall('.//count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count :  50\n",
      "Sum :  2553\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# extract all the comment/count values from the url and get the sum of all of them\n",
    "url = 'http://py4e-data.dr-chuck.net/comments_373797.xml'\n",
    "\n",
    "# get the content of the url as a string\n",
    "data = urllib.request.urlopen(url).read()\n",
    "\n",
    "# transform the string content into a xml tree\n",
    "tree = ET.fromstring(data)\n",
    "\n",
    "# find all count elements\n",
    "results = tree.findall('comments/comment')\n",
    "\n",
    "# extract the value of each count element and add it to the total\n",
    "total = 0\n",
    "count = 0\n",
    "for item in results :\n",
    "    #x = int(item.find('count').text)\n",
    "    #total = total + x\n",
    "    total += int(item.find('count').text)\n",
    "    count = count + 1\n",
    "print(\"Count : \",count)\n",
    "print(\"Sum : \",sum)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing JSON in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Chuck\n",
      "Hide: yes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#we have here one main Python dictionary\n",
    "data = '''\n",
    "{\n",
    "  \"name\" : \"Chuck\",\n",
    "  \"phone\" : {\n",
    "    \"type\" : \"intl\",\n",
    "    \"number\" : \"+1 734 303 4456\"\n",
    "   },\n",
    "   \"email\" : {\n",
    "     \"hide\" : \"yes\"\n",
    "   }\n",
    "}'''\n",
    "\n",
    "info = json.loads(data)#to load(s)trings\n",
    "print('Name:', info[\"name\"])\n",
    "print('Hide:', info[\"email\"][\"hide\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User count: 2\n",
      "Name Chuck\n",
      "Id 001\n",
      "Attribute 2\n",
      "Name Brent\n",
      "Id 009\n",
      "Attribute 7\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#a list of two dictionaries\n",
    "data = '''\n",
    "[\n",
    "  { \"id\" : \"001\",\n",
    "    \"x\" : \"2\",\n",
    "    \"name\" : \"Chuck\"\n",
    "  } ,\n",
    "  { \"id\" : \"009\",\n",
    "    \"x\" : \"7\",\n",
    "    \"name\" : \"Brent\"\n",
    "  }\n",
    "]'''\n",
    "\n",
    "info = json.loads(data)\n",
    "print('User count:', len(info))\n",
    "\n",
    "for item in info:\n",
    "    print('Name', item['name'])\n",
    "    print('Id', item['id'])\n",
    "    print('Attribute', item['x'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using application programming interfaces API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter location: print('Retrieved', len(data), 'characters')\n",
      "Retrieving http://py4e-data.dr-chuck.net/xml?address=print%28%27Retrieved%27%2C+len%28data%29%2C+%27characters%27%29&key=42\n",
      "Retrieved 107 characters\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<GeocodeResponse>\n",
      " <status>ZERO_RESULTS</status>\n",
      "</GeocodeResponse>\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-db37acddd4cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mlat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'geometry'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'location'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mlng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'geometry'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'location'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lng'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mlocation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'formatted_address'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#geoxml\n",
    "\n",
    "#Ann Arbor, MI, USA\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import xml.etree.ElementTree as ET\n",
    "import ssl\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "# api_key = 'AIzaSy___IDByT70'\n",
    "# https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = 'http://py4e-data.dr-chuck.net/xml?'\n",
    "else :\n",
    "    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/xml?'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1: break\n",
    "\n",
    "    parms = dict()\n",
    "    parms['address'] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "\n",
    "    data = uh.read()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "    print(data.decode())\n",
    "    tree = ET.fromstring(data)\n",
    "\n",
    "    results = tree.findall('result')\n",
    "    lat = results[0].find('geometry').find('location').find('lat').text\n",
    "    lng = results[0].find('geometry').find('location').find('lng').text\n",
    "    location = results[0].find('formatted_address').text\n",
    "\n",
    "    print('lat', lat, 'lng', lng)\n",
    "    print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter location: Ann Arbor, MI, USA\n",
      "Retrieving https://maps.googleapis.com/maps/api/geocode/json?address=Ann+Arbor%2C+MI%2C+USA&key=AIzaSy___IDByT70\n",
      "Retrieved 111 characters\n",
      "==== Failure To Retrieve ====\n",
      "{\n",
      "   \"error_message\" : \"The provided API key is invalid.\",\n",
      "   \"results\" : [],\n",
      "   \"status\" : \"REQUEST_DENIED\"\n",
      "}\n",
      "\n",
      "Enter location: \n"
     ]
    }
   ],
   "source": [
    "#geojson\n",
    "\n",
    "\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "#api_key = 'AIzaSy___IDByT70'\n",
    "# https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "else :\n",
    "    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'#after this point comes the entered location\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')#Ann Arbor, MI, USA\n",
    "    \n",
    "    if len(address) < 1: break #when we hit Enter it will break\n",
    "\n",
    "    parms = dict()\n",
    "    parms['address'] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "\n",
    "    if not js or 'status' not in js or js['status'] != 'OK':\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        continue\n",
    "\n",
    "    print(json.dumps(js, indent=4))\n",
    "\n",
    "    lat = js['results'][0]['geometry']['location']['lat']\n",
    "    lng = js['results'][0]['geometry']['location']['lng']\n",
    "    print('lat', lat, 'lng', lng)\n",
    "    location = js['results'][0]['formatted_address']\n",
    "    print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twurl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-79339ae9d74d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtwurl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mssl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# https://apps.twitter.com/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'twurl'"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import twurl\n",
    "import ssl\n",
    "\n",
    "# https://apps.twitter.com/\n",
    "# Create App and get the four strings, put them in hidden.py\n",
    "\n",
    "TWITTER_URL = 'https://api.twitter.com/1.1/statuses/user_timeline.json'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    print('')\n",
    "    acct = input('Enter Twitter Account:')\n",
    "    if (len(acct) < 1): break\n",
    "    url = twurl.augment(TWITTER_URL,\n",
    "                        {'screen_name': acct, 'count': '2'})\n",
    "    print('Retrieving', url)\n",
    "    connection = urllib.request.urlopen(url, context=ctx)\n",
    "    data = connection.read().decode()\n",
    "    print(data[:250])\n",
    "    headers = dict(connection.getheaders())\n",
    "    # print headers\n",
    "    print('Remaining', headers['x-rate-limit-remaining'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twurl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-79bc6a9b3109>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtwurl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mssl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'twurl'"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import twurl\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "# https://apps.twitter.com/\n",
    "# Create App and get the four strings, put them in hidden.py\n",
    "\n",
    "TWITTER_URL = 'https://api.twitter.com/1.1/friends/list.json'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    print('')\n",
    "    acct = input('Enter Twitter Account:')\n",
    "    if (len(acct) < 1): break\n",
    "    url = twurl.augment(TWITTER_URL,\n",
    "                        {'screen_name': acct, 'count': '5'})\n",
    "    print('Retrieving', url)\n",
    "    connection = urllib.request.urlopen(url, context=ctx)\n",
    "    data = connection.read().decode()\n",
    "\n",
    "    js = json.loads(data)\n",
    "    print(json.dumps(js, indent=2))\n",
    "\n",
    "    headers = dict(connection.getheaders())\n",
    "    print('Remaining', headers['x-rate-limit-remaining'])\n",
    "\n",
    "    for u in js['users']:\n",
    "        print(u['screen_name'])\n",
    "        if 'status' not in u:\n",
    "            print('   * No status found')\n",
    "            continue\n",
    "        s = u['status']['text']\n",
    "        print('  ', s[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import oauth\n",
    "import hidden\n",
    "\n",
    "# https://apps.twitter.com/\n",
    "# Create App and get the four strings, put them in hidden.py\n",
    "\n",
    "def augment(url, parameters):\n",
    "    secrets = hidden.oauth()\n",
    "    consumer = oauth.OAuthConsumer(secrets['consumer_key'],\n",
    "                                   secrets['consumer_secret'])\n",
    "    token = oauth.OAuthToken(secrets['token_key'], secrets['token_secret'])\n",
    "\n",
    "    oauth_request = oauth.OAuthRequest.from_consumer_and_token(consumer,\n",
    "                    token=token, http_method='GET', http_url=url,\n",
    "                    parameters=parameters)\n",
    "    oauth_request.sign_request(oauth.OAuthSignatureMethod_HMAC_SHA1(),\n",
    "                               consumer, token)\n",
    "    return oauth_request.to_url()\n",
    "\n",
    "\n",
    "def test_me():\n",
    "    print('* Calling Twitter...')\n",
    "    url = augment('https://api.twitter.com/1.1/statuses/user_timeline.json',\n",
    "                  {'screen_name': 'drchuck', 'count': '2'})\n",
    "    print(url)\n",
    "    connection = urllib.request.urlopen(url)\n",
    "    data = connection.read()\n",
    "    print(data)\n",
    "    headers = dict(connection.getheaders())\n",
    "    print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter location: http://py4e-data.dr-chuck.net/comments_373798.json\n",
      "Retrieving http://py4e-data.dr-chuck.net/comments_373798.json\n",
      "Retrieved 2717 characters\n",
      "Count: 50\n",
      "Sum: 2148\n"
     ]
    }
   ],
   "source": [
    "#Assignment JSON 1\n",
    "\n",
    "#http://py4e-data.dr-chuck.net/comments_373798.json\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "\n",
    "address = input('Enter location: ')\n",
    "    \n",
    "print('Retrieving', address)\n",
    "    \n",
    "uh = urllib.request.urlopen(address)\n",
    "    \n",
    "data = uh.read().decode()\n",
    "\n",
    "print('Retrieved', len(data), 'characters')\n",
    "\n",
    "info = json.loads(data)\n",
    "\n",
    "#print(json.dumps(info, indent=4))\n",
    "\n",
    "total = 0\n",
    "count = 0\n",
    "\n",
    "for item in info['comments']:\n",
    "    \n",
    "    total += (item['count'])  \n",
    "    count = count + 1\n",
    "    \n",
    "print(\"Count:\", count)\n",
    "print(\"Sum:\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://py4e-data.dr-chuck.net/json?address=Pondicherry+University&key=42\n",
      "Retrieved 2226 characters\n",
      "Place_Id: ChIJWXnloXZhUzoRul7TO_m8wvk\n"
     ]
    }
   ],
   "source": [
    "#Assignment JSON 2\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "\n",
    "api_key = 42\n",
    "serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "address = 'Pondicherry University'\n",
    "    \n",
    "\n",
    "parms = dict()\n",
    "parms['address'] = address\n",
    "parms['key'] = api_key\n",
    "\n",
    "url = serviceurl + urllib.parse.urlencode(parms)\n",
    "print('Retrieving', url)\n",
    "\n",
    "uh = urllib.request.urlopen(url)\n",
    "data = uh.read().decode()\n",
    "\n",
    "print('Retrieved', len(data), 'characters')\n",
    "\n",
    "js = json.loads(data)\n",
    "    \n",
    "\n",
    "#print(json.dumps(js, indent=4))\n",
    "    \n",
    "place_id = js['results'][0]['place_id']\n",
    "\n",
    "print('Place_Id:', place_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
